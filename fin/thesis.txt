Polyphonic Transcription by Heuristic Inference

presentation/article due may 7

polytransinf
> particle filter that samples from likelihood
 sample x[t] from P(x[t]|y[t]) => weight by P(x[t]|x[t-1])
> sparse approximation
 keep K[=subspace] most likely notes from heuristic of audio
> greedy search
 choose most likely b[=breadth] notes => subtract audio from signal d[=depth] times
 A* ?
~ h(y)
 = data => deterministic efficient function => list states
 : heuristic
 > sort notes by energy of fundamental
 > sort notes by value from nmf
test
 agree with exact inference [eg max-product forward-backward] on subset [eg 2^12 not 2^88]

graphical model for polyphonic transcription
the graph
 factorial HMM
 shrink space by sparsity
the inference
 approx
the learning
 model noise and create noised audio
 learn stickiness etc

-  -  -  -  -  -  -  -  -  -  -  -  -  -  -

particle filter ~>
p : P
q : P
p* ∝ p : can eval
q* ∝ q : can eval : can sample
K = |particles|

weight of particle filter , w[k][t]
= f(x[t]|x[t-1]) * g(y[t]|x[t]) / q(x[t]|x[t-1],y[t])
q = g -> w = f
f = ∏ Ber
g = heuristic = fft | nmf | search

approx on space ~ exact on subspace

google scholar . polyphonic transcription cemgil

gd, nmf, particle filter, neural network  on  sam-gen notes-sounds

models
sumproduct
importance sampling
rejection sampling
. HMM
. kalman filter
extended kalman filter
unscented kalman filter
particle filter
MCMC
metropolis-hastings sampling
gibbs sampling
variational ??

noise
. this is it, i can fuckin feelit!
<~ P(y[t] | x[t])
: gaussian
: chi-squared
: noncentral chi-squared

new
greedy
lateral inhibition
shrink space
ADSR

seed _ with GD / NMF

ADSR
-> P(x[t,i] | x[t-1,i])
-> + space
next-or-stay cycle transition matrix
 each row : P  ~  ∑x T(x|_) = 1
 matrix
     A   D   S   R
 A [ a   1-a 0   0
 D   0   d   1-d 0
 S   0   0   s   1-s
 R   1-r 0   0   r ]

notes in chord  are conditionally independent  given prev chord and sparsity

 monte carlo tree search  would be  do this for several start notes, say top[by some "likely for note in be in signal" metric, probably fundamental loudness] two notes, each time, exponential growth but small base[≈ number of likely notes; not possible notes, which would be 88 for piano] and small exponent[≈ depth of search, after you take away enough notes [below 5, i think], you can do forward-backward HMM].

 solving this problem is like biasing the search / reranking the notes / shrinking the space by some heuristic; then it does exact inference on the "likely ones" or subsets and stops before trying everything.

 avoid recomputation, which are likely, since we do the "most likely" then the "next most likely". so like sampling without replacement.
  eg 7 notes
   rem F# => rem C  =  rem C => rem F#
 : solve one problem => solve several relaxations
 prob. subtracting note from audio subtracts the "true note" but leaves the noise
  -> if you greedily guess-and-simpl too many times, you will get a noisier and noisier signal [unless it cancels out?]
  in the limit, you remove all the notes noise all you have is noise (assuming the model is right, eg that the actual and assumed instruments coincide)
 prob. multiple instruments . same note on diff instruments share the same fundamental . must try them all
 top k ~ sample k
  instead of deterministically taking the k loudest fundamentals, we might take k samples from some distribution on notes given the audio[eg the fundamentals] . isnt this worse? not strictly, because we do sacrifice exploitation for more exploration

shrink space by sparsity
eg n=88 k=10
 2^88 => 88 C 10
 . still too big <- doesnt scale to multiple instruments
 . k=10 is sensible but n=88 cant model bands

prob. loud base and soft overtone
 eg loud A2 . soft E3 . E3 is the loudest overtone of A2
soln. look at the overtone's overtones
 do does a fundamental share many overtones with its own overtones? probably
soln. reweight audio energy by human perception
 . humans are less sensitive to bass so it might dominate the audio while the treble dominates the music, so a false negative will be obviously wrong

eval / test / compare / results
compare just emissions  to emissions and transitions
 transitions are sticky and thus smooth
 ADSR would be great for this
time
 run time , test time
  want realtime[≤ 1 second compute per second input]
 train time
data
 > midi data
  midi ==> piano roll , truth
  midi ==> audio == input
  audio + noise => infer by model ==> output
  compare truth  to output
 > real data
  noisy room + ok recorder + human playing + fast music + scriabin etude + with pedal + more varied dynamics + more varied rhythm + more varied attack
accuracy
 objectively/automatically.
  diff binary matrices[where cols=moments rows=notes]  ie  output notes v true notes
  count number of notes by horizontal flood fill
 subjectively/manually.
  check that notes dont have 'holes' . ie small false negatives . which would mean that the transitions arent sticky enough
  check that entire notes are not missing . ie big false negatives . which would mean that sparsity is too strong or a big problem with the model
  check for 'dots' . ie small false positives . meaning not enough sparsity or bad transitions
  check for wrong notes [not transposes] . ie big false positives . big problem with the model or too much noise that isnt being dealt with
  check that the notes are not transposed . which would mean a problem in the basis . ie misspecified, linearly dependent, etc
  look for other things
  check that its not obviously better at bass over treble . which means i might need to logarithmically rescale energies to the sensitivity of the human ear
  check that its not missing short notes . i might need to shrink the window size or tune the difference between noise and short notes [hopefully noise is much briefer]

real  v  midi
more noise
 <- noisy room
 <- worse recorder
more notes
 <- with pedal
diff notes
 <- piano tuning
 <- human playing which
  -> more varied attack
  ? -> more varied dynamics
  ? -> more varied rhythm
