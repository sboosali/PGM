-  -  -  -  -  -  -  -  -  -  -  -  -  -  -
.proposal

either
new GM
new algorithm
new data

    Identify a graphical model suitable for a new application area, and explore baseline learning algorithms
    Propose, develop, and experimentally test a new type of learning algorithm for some existing graphical model
    Experimentally compare different models or algorithms on an interesting, novel dataset

proposal
1..3pg
use the NIPS LaTeX style file
why this problem?
what's new? what's out there? (3 papers)
how to eval?
what's the plan?
what's the GM

    A clear description of the problem or application you intend to address. Why is it worth studying?
    A discussion of related work, including references to at least three relevant research articles. Which aspects of your project are novel?
    An experimental evaluation protocol. How will you know that you've succeeded?
    A concrete plan for accomplishing your project by the end of the course. What are the biggest challenges?
    A figure illustrating a graphical model which plays a role in your project. We recommend creating such figures in a vector drawing program, such as Adobe Illustrator, Inkscape, or Xfig.

-  -  -  -  -  -  -  -  -  -  -  -  -  -  -

Polyphonic Transcription by Heuristic Inference

presentation/article due may 7

google scholar . polyphonic transcription cemgil

gd, nmf, particle filter, neural network  on  sam-gen notes-sounds

-  -  -  -  -  -  -  -  -  -  -  -  -  -  -
ideas

models
sumproduct
importance sampling
rejection sampling
. HMM
. kalman filter
extended kalman filter
unscented kalman filter
particle filter
MCMC
metropolis-hastings sampling
gibbs sampling
variational ??

noise
. this is it, i can fuckin feelit!
<~ P(y[t] | x[t])
: gaussian
: chi-squared
: noncentral chi-squared

new
greedy
lateral inhibition
shrink space
ADSR

learning
how the hell do i do learning on this gmod?

seed _ with GD / NMF

ADSR
-> P(x[t,i] | x[t-1,i])
-> + space
next-or-stay cycle transition matrix
 each row : P  ~  ∑x T(x|_) = 1
 matrix
     A   D   S   R
 A [ a   1-a 0   0
 D   0   d   1-d 0
 S   0   0   s   1-s
 R   1-r 0   0   r ]

lateral inhibition
must be continuous
y' interpretation
additive | multiplicative 
~> noise model
x' interpretation
 x ∈ {0 1}^88
 x' ∈ R^88
 y ∈ R^1024
 what if deterministic? nope
  p(x'[t] | x[t]) => x'[t] == f(x[t])
  invert f if can
 p(y[t] | x'[t]) p(x'[t] | x[t]) p(x[t] | x[t-1])
  black box away x' by _ ->  reduces to HMM  ?
   marginalization [which elims vars] is too hard
   gibbs

HMM
p(x[t] | y[t]) ∝ p(y[t] | x[t]) p(x[t] | x[t-1])
sample from p(x[t] | x[t-1])
weight by p(y[t] | x[t])
good for 1=>1 1=>0 0=>0
bad for 0=>1
the asymmetry, both the good and the bad, is by sparsity of 1s
silence can go to any sound, to many sounds, so which?

x[t]
: continuous
in [0,1]^88
 find min,max,mean energies of signal prefix
 only normalize if audio freqs are all the same (ie zero)
  or normalzie to the degree which it coincides

y ==> lateral inhibition  ~  mcts  ~  sample x[t] from P(x[t]|y[t]) not P(x[t]|x[t-1])

[old] lateral inhibition
 preprocess audio
 -> P(x[t,i] | x[t-1,i] , x[t-1,j] where j ∈ overtones(i) , x[t-1,k] where k ∈ neighbors(i))
 -> - space
 -> - false positives
 . if you hear a note, its overtones[? nearby] are less likely
 . the transition lowers the probability of a soft overtone note given a loud fundamental note. but you can still get octaves, ifths, etc if the emission is strong.
 . only transition -> no octaves -> false negatives
 . only emission -> yes shadows -> false positives
 . either learns params after seeding with (or just set to) the nontrivial overtones of each note
noise params



model
|samples per moment| = ??
|moments per what| = ??



second if probability is unnormalized reciprocal search length?
then its a probability technically-syntactically but not meaningfully-semantically. if you cant interpret it anyway, just do a regularized function approximation.
A* search over continue melody or chord progression
greedy
linearizes exponential
sample |notes| => sample one next note given all curr notes => samples until you get |notes| notes
 notes in chord  are conditionally independent  given prev chord and sparsity
init silence ~ sparsity
 src cemgil

greedy
humans probably do something greedy. like parsing natlang? 
guess some note (eg whose fundamental is the loudest frequency the signal) => subtract it (i.e. note = fundamental + overtones + under?tones) from signal => rinse and repeat
 monte carlo tree search  would be  do this for several start notes, say top[by some "likely for note in be in signal" metric, probably fundamental loudness] two notes, each time, exponential growth but small base[≈ number of likely notes; not possible notes, which would be 88 for piano] and small exponent[≈ depth of search, after you take away enough notes [below 5, i think], you can do forward-backward HMM].
 solving this problem is like biasing the search / reranking the notes / shrinking the space by some heuristic; then it does exact inference on the "likely ones" or subsets and stops before trying everything.
 avoid recomputation, which are likely, since we do the "most likely" then the "next most likely". so like sampling without replacement.
  eg 7 notes
   rem F# => rem C  =  rem C => rem F#
 : solve one problem => solve several relaxations
 prob. subtracting note from audio subtracts the "true note" but leaves the noise
  -> if you greedily guess-and-simpl too many times, you will get a noisier and noisier signal [unless it cancels out?]
  in the limit, you remove all the notes noise all you have is noise (assuming the model is right, eg that the actual and assumed instruments coincide)
 prob. multiple instruments . same note on diff instruments share the same fundamental . must try them all
 top k ~ sample k
  instead of deterministically taking the k loudest fundamentals, we might take k samples from some distribution on notes given the audio[eg the fundamentals] . isnt this worse? not strictly, because we do sacrifice exploitation for more exploration

shrink space
sparsity
 2^88 => 88 C 10
  still too big
  doesnt scale to multiple instruments  ie  k=10 is small and sensible but n=88 cant model bands
locality
 88 C 10 => ??
greedy



P(x[t,f] | x[t-1,f]) => P(x[t,f] | x[t-1,:])
 means note can depend on any prev note
 good. allows music priors
 bad. explodes space
 -> lateral inhibition


polytrans ~ particle filter
p : P
q : P
p* : can eval
q* : can eval : can sample



what if, instead of sampling from the transition/dynamic P(x[t] | x[t-1]) and weighting by the emission/measurement P(x[t] | y[t]), you sample from the emissiono and weight by the transition! that seems to be more what humans do. e.g. hear some sound, think it could sound like one of a several different notes/intervals/chords, figureout which is most likely given previous melody/harmony. the samples arent "chained" anymore [i.e.  x[t+1] ~> x[t] ~> x[t-1]  =>  x[t] ~> y[t] ... ], but still dependent. [must be chained for stickiness, ADSR transitions]
is this called something else? is this ok? what about both, or cant we mix particles like that?

what if, the dynamics [ie x[t,i] ~> x[t-1, :]] dont really matter. what if its just the measurement [ie x[t,i] ~> y[t,:]] and the ADSR [ie x[t, i] ~> x[t-1, i]]that can infer the right notes. this is like saying "less music theory, more signal processing" or "local not global" or "y[t] does not quite disambiguate x[t], which is why it's an HMM, but it does really constrains it". 
isnt this what cemgil says?


put more signal processing / noise model / etc in P(x[t] | y[t])
its about samples, not seconds. its about ADSR, not music theory. humans (with perfect pitch) can figure out what several notes were all played within a second [is this true?]. 

ADSR
cant model legato/vibrato
 src cemgil

prob? loud base and soft overtone
 eg loud A2 . soft E3 . E3 is the loudest overtone of A2
soln. look at the overtone's overtones
 do does a fundamental share many overtones with its own overtones? probably


eval / test / compare / results
compare just emissions  to emissions and transitions
 transitions are sticky and thus smooth
 ADSR would be great for this
time
 run time , test time
  want realtime[≤ 1 second compute per second input]
 train time
data
 > midi data
  midi ==> piano roll , truth
  midi ==> audio == input
  audio + noise => infer by model ==> output
  compare truth  to output
 > real data
  noisy room + ok recorder + human playing + fast music + scriabin etude + with pedal + more varied dynamics + more varied rhythm + more varied attack
accuracy
 objectively/automatically.
  diff binary matrices[where cols=moments rows=notes]  ie  output notes v true notes
  count number of notes by horizontal flood fill
 subjectively/manually.
  check that notes dont have 'holes' . ie small false negatives . which would mean that the transitions arent sticky enough
  check that entire notes are not missing . ie big false negatives . which would mean that sparsity is too strong or a big problem with the model
  check for 'dots' . ie small false positives . meaning not enough sparsity or bad transitions
  check for wrong notes [not transposes] . ie big false positives . big problem with the model or too much noise that isnt being dealt with
  check that the notes are not transposed . which would mean a problem in the basis . ie misspecified, linearly dependent, etc
  look for other things
  check that its not obviously better at bass over clef . which means i might need to logarithmically rescale energies to the sensitivity of the human ear
  check that its not missing short notes . i might need to shrink the window size or tune the difference between noise and short notes [hopefully noise is much briefer]


real  v  midi
more noise
 <- noisy room
 <- worse recorder
more notes
 <- with pedal
diff notes
 <- piano tuning
 <- human playing which
  -> more varied attack
  ? -> more varied dynamics
  ? -> more varied rhythm
 

graphical model for polyphonic transcription
the graph
 shrink space
 sparsity
the inference
the learning
 model noise and create noised audio
 learn stickiness etc


